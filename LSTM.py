import torch.nn as nn
import torch
import matplotlib.pyplot as plt
from torch.optim import Adam
from IPython.display import clear_output
import gc
from torch.cuda import memory_allocated, empty_cache
import numpy as np

if torch.cuda.is_available() == True:
    device = 'cuda:0'
    print('GPU ì‚¬ìš© ê°€ëŠ¥')
else:
    device = 'cpu'
    print('GPU ì‚¬ìš© ë¶ˆê°€ëŠ¥')

length = 60


class skeleton_LSTM(nn.Module):
    def __init__(self):
        super(skeleton_LSTM, self).__init__()
        self.lstm1 = nn.LSTM(input_size=24, hidden_size=128, num_layers=1, batch_first=True)
        self.lstm2 = nn.LSTM(input_size=128, hidden_size=256, num_layers=1, batch_first=True)
        self.lstm3 = nn.LSTM(input_size=256, hidden_size=512, num_layers=1, batch_first=True)
        self.dropout1 = nn.Dropout(0.3)
        # self.batch_norm1 = nn.BatchNorm1d(256)
        self.lstm4 = nn.LSTM(input_size=512, hidden_size=256, num_layers=1, batch_first=True)
        self.lstm5 = nn.LSTM(input_size=256, hidden_size=128, num_layers=1, batch_first=True)
        self.lstm6 = nn.LSTM(input_size=128, hidden_size=64, num_layers=1, batch_first=True)
        self.dropout2 = nn.Dropout(0.3)
        # self.batch_norm2 = nn.BatchNorm1d(64)
        self.lstm7 = nn.LSTM(input_size=64, hidden_size=32, num_layers=1, batch_first=True)
        self.fc = nn.Linear(32, 4)
        # self.lstm_1=nn.LSTM(input_size=32,hidden_size=64, num_layers=1, batch_first=True)
        # self.dropout_1 = nn.Dropout(0.3)
        # self.batch_norm1 = nn.BatchNorm1d(64)

        # self.lstm_2 = nn.LSTM(input_size=64, hidden_size=32, num_layers=1, batch_first=True)
        # self.dropout_2 = nn.Dropout(0.5)
        # self.batch_norm2 = nn.BatchNorm1d(16)
        # self.lstm_3 = nn.LSTM(input_size=128, hidden_size=64, num_layers=1, batch_first=True)
        # self.lstm_4 = nn.LSTM(input_size=64, hidden_size=32, num_layers=1, batch_first=True)
        # self.fc_1 = nn.Linear(32, 2)


    def forward(self, x):
        x, _ = self.lstm1(x)
        x, _ = self.lstm2(x)
        x, _ = self.lstm3(x)
        x = self.dropout1(x)
        # x = self.batch_norm1(x.permute(0, 2, 1)).permute(0, 2, 1)
        x, _ = self.lstm4(x)
        x, _ = self.lstm5(x)
        x, _ = self.lstm6(x)
        x = self.dropout2(x)
        # x = self.batch_norm2(x.permute(0, 2, 1)).permute(0, 2, 1)
        x, _ = self.lstm7(x)
        x = self.fc(x[:, -1, :])
        # x, _ = self.lstm_1(x)
        # x = self.dropout_1(x)
        # x = self.batch_norm1(x.permute(0, 2, 1)).permute(0, 2, 1)

        # x, _ = self.lstm_2(x)
        # x = self.dropout_2(x)
        # x = self.batch_norm2(x.permute(0, 2, 1)).permute(0, 2, 1)
        # x = self.dropout_2(x)
        # x, _ = self.lstm_3(x)
        # x, _ = self.lstm_4(x)
        # x = self.fc_1(x[:, -1, :])
        return x


def init_model():
    plt.rc('font', size=10)
    global net, loss_fn, optim
    net = skeleton_LSTM().to(device)
    loss_fn = nn.CrossEntropyLoss()
    optim = Adam(net.parameters(), lr=0.0001)


def init_epoch():
    global epoch_cnt
    epoch_cnt = 0


def init_log():
    plt.rc('font', size=10)
    global log_stack, iter_log, tloss_log, tacc_log, vloss_log, vacc_log, time_log
    iter_log, tloss_log, tacc_log, vloss_log, vacc_log, time_log, log_stack = [], [], [], [], [], [], []


def record_train_log(t_loss, t_acc, t_time):
    time_log.append(t_time)
    tloss_log.append(t_loss)
    tacc_log.append(t_acc)
    iter_log.append(epoch_cnt)


def record_valid_log(v_loss, v_acc):
    vloss_log.append(v_loss)
    vacc_log.append(v_acc)


def last(log_list):
    if len(log_list) > 0:
        return log_list[len(log_list) - 1]
    else:
        return -1


def print_log():
    # í•™ìŠµ ì¶”ì´ ì¶œë ¥

    # ì†Œìˆ«ì  3ìë¦¬ ìˆ˜ê¹Œì§€ ì¡°ì ˆ
    train_loss = round(float(last(tloss_log)), 10)
    train_acc = round(float(last(tacc_log)), 10)
    val_loss = round(float(last(vloss_log)), 10)
    val_acc = round(float(last(vacc_log)), 10)
    time_spent = round(float(last(time_log)), 10)

    log_str = 'Epoch: {:3} | T_Loss {:5} | T_acc {:10} | V_Loss {:5} | V_acc. {:5} | \
ğŸ•’ {:5}'.format(last(iter_log), train_loss, train_acc, val_loss, val_acc, time_spent)

    log_stack.append(log_str) # í”„ë¦°íŠ¸ ì¤€ë¹„

    # í•™ìŠµ ì¶”ì´ ê·¸ë˜í”„ ì¶œë ¥
    hist_fig, loss_axis = plt.subplots(figsize=(10, 3), dpi=99) # ê·¸ë˜í”„ ì‚¬ì´ì¦ˆ ì„¤ì •
    hist_fig.patch.set_facecolor('white') # ê·¸ë˜í”„ ë°°ê²½ìƒ‰ ì„¤ì •

    # Loss Line êµ¬ì„±
    loss_t_line = plt.plot(iter_log, tloss_log, label='Train Loss', color='red', marker='o')
    loss_v_line = plt.plot(iter_log, vloss_log, label='Valid Loss', color='blue', marker='s')
    loss_axis.set_xlabel('epoch')
    loss_axis.set_ylabel('loss')

    # Acc. Line êµ¬ì„±
    acc_axis = loss_axis.twinx()
    acc_t_line = acc_axis.plot(iter_log, tacc_log, label='Train Acc.', color='red', marker='+')
    acc_v_line = acc_axis.plot(iter_log, vacc_log, label='Valid Acc.', color='blue', marker='x')
    acc_axis.set_ylabel('accuracy')

    # ê·¸ë˜í”„ ì¶œë ¥
    hist_lines = loss_t_line + loss_v_line + acc_t_line + acc_v_line # ìœ„ì—ì„œ ì„ ì–¸í•œ pltì •ë³´ë“¤ í†µí•©
    loss_axis.legend(hist_lines, [l.get_label() for l in hist_lines]) # ìˆœì„œëŒ€ë¡œ ê·¸ë ¤ì£¼ê¸°
    loss_axis.grid() # ê²©ì ì„¤ì •
    plt.title('Learning history until epoch {}'.format(last(iter_log)))
    plt.draw()

    # í…ìŠ¤íŠ¸ ë¡œê·¸ ì¶œë ¥
    clear_output(wait=True)
    plt.show()
    for idx in reversed(range(len(log_stack))): # ë°˜ëŒ€ë¡œ sort ì‹œì¼œì„œ ì¶œë ¥
        print(log_stack[idx])


def clear_memory():
    if device != 'cpu':
        empty_cache()
    gc.collect()


def epoch(data_loader, mode = 'train'):
    global epoch_cnt

    # ì‚¬ìš©ë˜ëŠ” ë³€ìˆ˜ ì´ˆê¸°í™”
    iter_loss, iter_acc, last_grad_performed = [], [], False

    # 1 iteration í•™ìŠµ ì•Œê³ ë¦¬ì¦˜(forë¬¸ì„ ë‚˜ì˜¤ë©´ 1 epoch ì™„ë£Œ)
    for _data, _label in data_loader:
        data, label = _data.to(device), _label.type(torch.LongTensor).to(device)

        # 1. Feed-forward
        if mode == 'train':
            net.train()
        else:
            # í•™ìŠµë•Œë§Œ ì“°ì´ëŠ” Dropout, Batch Mormalizationì„ ë¯¸ì‚¬ìš©
            net.eval()

        result = net(data) # 1 Batchì— ëŒ€í•œ ê²°ê³¼ê°€ ëª¨ë“  Classì— ëŒ€í•œ í™•ë¥ ê°’ìœ¼ë¡œ
        _, out = torch.max(result, 1) # resultì—ì„œ ìµœëŒ€ í™•ë¥ ê°’ì„ ê¸°ì¤€ìœ¼ë¡œ ì˜ˆì¸¡ class ë„ì¶œ

        # 2. Loss ê³„ì‚°
        loss = loss_fn(result, label) # GT ì™€ Label ë¹„êµí•˜ì—¬ Loss ì‚°ì •
        iter_loss.append(loss.item()) # í•™ìŠµ ì¶”ì´ë¥¼ ìœ„í•˜ì—¬ Lossë¥¼ ê¸°ë¡

        # 3. ì—­ì „íŒŒ í•™ìŠµ í›„ Gradient Descent
        if mode == 'train':
            optim.zero_grad() # ë¯¸ë¶„ì„ í†µí•´ ì–»ì€ ê¸°ìš¸ê¸°ë¥´ ì´ˆê¸°í™” for ë‹¤ìŒ epoch
            loss.backward() # ì—­ì „íŒŒ í•™ìŠµ
            optim.step() # Gradient Descent ìˆ˜í–‰
            last_grad_performed = True # forë¬¸ ë‚˜ê°€ë©´ epoch ì¹´ìš´í„° += 1

        # 4. ì •í™•ë„ ê³„ì‚°
        acc_partial = (out == label).float().sum() # GT == Label ì¸ ê°œìˆ˜
        acc_partial = acc_partial / len(label) # ( TP / (TP + TN)) í•´ì„œ ì •í™•ë„ ì‚°ì¶œ
        iter_acc.append(acc_partial.item()) # í•™ìŠµ ì¶”ì´ë¥¼ ìœ„í•˜ì—¬ Acc. ê¸°ë¡

    # ì—­ì „íŒŒ í•™ìŠµ í›„ Epoch ì¹´ìš´í„° += 1
    if last_grad_performed:
        epoch_cnt += 1

    clear_memory()

    # lossì™€ accì˜ í‰ê· ê°’ for í•™ìŠµì¶”ì´ ê·¸ë˜í”„, ëª¨ë“  GTì™€ Labelê°’ for ì»¨í“¨ì „ ë§¤íŠ¸ë¦­ìŠ¤
    return np.average(iter_loss), np.average(iter_acc)

def epoch_not_finished():
    # return epoch_cnt < training.maximum_epoch
    return epoch_cnt < 50
